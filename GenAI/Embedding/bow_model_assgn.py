# -*- coding: utf-8 -*-
"""BoW_Model_Assgn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13m-AFl1J3070mIguzG60N4D1AEpQl838

## Bag of Words - Word Embedding Model
"""

# Importing all the necessary libraries
import urllib.request                                       # For handling URL requests to download the text for the book
import nltk                                                 # Natural Language Toolkit (NLTK)
from nltk import word_tokenize                              # Word tokenizer from NLTK
import string                                               # String library to handle punctuation
from nltk.corpus import stopwords                           # The stopwords corpus
from nltk.stem.porter import PorterStemmer                  # For stemming the tokens
from nltk.probability import FreqDist                       # To calculate word frequency distribution
from nltk import sent_tokenize                              # To tokenize the text into sentences
import numpy as np                                          # For numerical operations
from sklearn.feature_extraction.text import CountVectorizer # To convert the text data into a word count vector

# URL of the text file to be downloaded
url = "https://www.gutenberg.org/files/2701/2701-0.txt"
# Open the URL and read the contents of the file
file = urllib.request.urlopen(url)

# Decode each line of the text file from bytes to string and store in a list
text = [line.decode('utf-8') for line in file]

# Join the list into a single string
text = ''.join(text)

# Display a substring of the text (from character 7600 to 8000) for a quick check
text[7600:8000]

# Download the 'punkt' tokenizer model
nltk.download('punkt')

# Tokenize the text
tokens = word_tokenize(text)

"""Clean the tokens by removing punctuation and non-alphabetic tokens"""

# Keep only alphabetic tokens
tokens = [word for word in tokens if word.isalpha()]

# Create a translation table that maps punctuation to None (to remove punctuation)
table = str.maketrans('', '', string.punctuation)

# Remove punctuation from tokens
tokens = [w.translate(table) for w in tokens]

# Convert all tokens to lowercase
tokens = [word.lower() for word in tokens]

"""Remove stopwords and stem the tokens"""

# Download the stopwords list
nltk.download('stopwords')

# Define the set of English stopwords
stop_words = set(stopwords.words('english'))

# Filter out stopwords from the tokens
tokens = [w for w in tokens if not w in stop_words]

# Define the set of English stopwords
stop_words = set(stopwords.words('english'))

# Filter out stopwords from the tokens
tokens = [w for w in tokens if not w in stop_words]

# Stemming the tokens
porter = PorterStemmer()

# Apply stemming to the tokens
tokens = [porter.stem(word) for word in tokens]

# Display a subset of the tokens (from index 200 to 222) for a quick check
tokens[200:222]

"""Calculate word frequency distribution"""

# Create a frequency distribution of the tokens
word_counts = FreqDist(tokens)

# Display the word frequency distribution
word_counts

# Define the top N words to include in the vocabulary
top = 500

# Get the most common words in the corpus (top 500)
vocabulary = word_counts.most_common(top)

# Display the top 10 most common words in the vocabulary
vocabulary[:10]

# Define the size of the vocabulary
voc_size = len(vocabulary)

# Initialize a document vector with zeros (of the same length as the vocabulary)
doc_vector = np.zeros(voc_size)

# Create a word vector (list of tuples containing index and word count)
word_vector = [(idx, word_counts[word[0]]) for idx, word in enumerate(vocabulary) if word[0] in word_counts.keys()]

# Display the 11th item in the word vector
word_vector[10]

# Populate the document vector with the word counts
for idx, count in word_vector:
    doc_vector[idx] = count

# Display the document vector
doc_vector

"""Tokenize the text into sentences"""

# Extract a subset of sentences (703rd to 706th sentence) from the text
docs = sent_tokenize(text)[703:706]

# Display the extracted sentences
docs

"""Convert the text data into a word count vector"""

# Initialize the CountVectorizer with English stopwords
count_vectorizer = CountVectorizer(stop_words='english')

# Fit the CountVectorizer to the selected sentences and transform them into a word count vector
word_count_vector = count_vectorizer.fit_transform(docs)

# Display the shape of the word count vector (number of documents, number of features)
word_count_vector.shape

# Convert the word count vector to an array and display it
word_count_vector.toarray()

#Get and display the feature names (vocabulary) learned by the CountVectorizer
count_vectorizer.get_feature_names_out()