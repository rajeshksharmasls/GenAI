# -*- coding: utf-8 -*-
"""GloVe_Model_Assgn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/125fXxuEOnhMtmxTiiT4Otxbv0eKlidIo
"""



"""## Global Vectors for Word Representation (GloVe) - Word Embedding"""

!pip install pyspellchecker

# Importing all the necessary libraries
import pandas as pd                                         # For data handling
import numpy as np                                          # For working with arrays and matrices
import os                                                   # For file handling
import re                                                   # For preprocessing
from time import time                                       # To time our operations
from collections import defaultdict                         # For word frequency
import spacy                                                # For preprocessing
import logging                                              # Setting up the loggings to monitor gensim
import nltk                                                 # Natural Language Toolkit (NLTK)
import matplotlib.pyplot as plt                             # For plotting graphs
import seaborn as sns                                       # For creating attractive and informative statistical graphics
from nltk.corpus import stopwords                           # For list of common words to be filtered out in text processing.
from nltk.util import ngrams                                # To generate n-grams from text
from sklearn.feature_extraction.text import CountVectorizer # To convert a collection of text documents to a matrix of token counts
from collections import defaultdict                         # To create dictionaries with default values
from collections import Counter                             # To count the frequency of elements in a collection
import re                                                   # For regular expressions, useful for text processing
from nltk.tokenize import word_tokenize                     # To split text into individual words (tokens)
import gensim                                               # For topic modeling and document similarity, often used with word embeddings
import string                                               # For string operations, often used for text cleaning
from keras.preprocessing.sequence import pad_sequences      # To pad sequences to the same length, often used for preparing data for neural networks
from tqdm import tqdm                                       # For creating progress bars, useful for long-running loops
from keras.models import Sequential                         # To build neural networks
from spellchecker import SpellChecker                       # To correct spelling errors in text
from tensorflow.keras.preprocessing.text import Tokenizer   # To vectorize a text corpus by turning text into sequences of integers
from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D # Specific layers for building neural networks
from keras.initializers import Constant                           # For initializing the weights of a layer to a constant value
from sklearn.model_selection import train_test_split              # For evaluating the performance of your model on unseen data
from keras.optimizers import Adam                                 # Optimization algorithm that adjusts the learning rate during training

for dirname, _, filenames in os.walk('/content/GloVe'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# Read the train and test files
train_df = pd.read_csv("/content/GloVe/train.csv")
test_df = pd.read_csv("/content/GloVe/test.csv")

# Defining hyperparameters
settings = {
'window_size': 2,# context window +- center word
'n': 10,# dimensions of word embeddings, also refer to size of hidden layer
'epochs': 50,# number of training epochs
'learning_rate': 0.01# learning rate
}

# Dataframes for train and test files
train_df = pd.DataFrame(train_df)
test_df = pd.DataFrame(test_df)

# Dimensions for train file
train_df.shape

# Dimensions for test file
test_df.shape

# Set the style of plots to 'ggplot' (a popular plotting style) using Matplotlib.
plt.style.use('ggplot')

# Load the set of English stopwords from NLTK to filter out common words.
nltk.download('stopwords')
stop = set(stopwords.words('english'))

tweet= pd.read_csv('/content/GloVe/train.csv')
test= pd.read_csv('/content/GloVe/test.csv')
tweet.head(3)

x=tweet.target.value_counts()

sns.barplot(x)
plt.gca().set_ylabel('samples')

#Number of characters in the tweet
fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
tweet_len=tweet[tweet['target']==1]['text'].str.len()
ax1.hist(tweet_len,color='red')
ax1.set_title('disaster tweets')
tweet_len=tweet[tweet['target']==0]['text'].str.len()
ax2.hist(tweet_len,color='green')
ax2.set_title('Not disaster tweets')
fig.suptitle('Characters in tweets')
plt.show()

# Number of words in the tweet

fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))
tweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))
ax1.hist(tweet_len,color='red')
ax1.set_title('disaster tweets')
tweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))
ax2.hist(tweet_len,color='green')
ax2.set_title('Not disaster tweets')
fig.suptitle('Words in a tweet')
plt.show()

# Create a list of words from the text data

def create_corpus(target):
    corpus=[]

    for x in tweet[tweet['target']==target]['text'].str.split():
        for i in x:
            corpus.append(i)
    return corpus

# Common stop words in the tweet

corpus=create_corpus(0)

dic=defaultdict(int)
for word in corpus:
    if word in stop:
        dic[word]+=1

top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]

x,y=zip(*top)
plt.bar(x,y)

corpus=create_corpus(1)

dic=defaultdict(int)
for word in corpus:
    if word in stop:
        dic[word]+=1

top=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10]

x,y=zip(*top)
plt.bar(x,y)

# Cleaning
df=pd.concat([tweet,test])
df.shape

# Removing the URL from the text

example="New competition launched :https://www.kaggle.com/c/nlp-getting-started"
def remove_URL(text):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'',text)

remove_URL(example)

df['text']=df['text'].apply(lambda x : remove_URL(x))

# Removing HTML tags

example = """<div>
<h1>Real or Fake</h1>
<p>Kaggle </p>
<a href="https://www.kaggle.com/c/nlp-getting-started">getting started</a>
</div>"""
def remove_html(text):
    html=re.compile(r'<.*?>')
    return html.sub(r'',text)
print(remove_html(example))

df['text']=df['text'].apply(lambda x : remove_html(x))

# Removing emojis from the text

def remove_emoji(text):
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

remove_emoji("Omg another Earthquake ðŸ˜”ðŸ˜”")

df['text']=df['text'].apply(lambda x: remove_emoji(x))

# Removing punctuations

def remove_punct(text):
    table=str.maketrans('','',string.punctuation)
    return text.translate(table)

example="I am a #king"
print(remove_punct(example))

df['text']=df['text'].apply(lambda x : remove_punct(x))

# Spelling correction

spell = SpellChecker()
def correct_spellings(text):
    corrected_text = []
    misspelled_words = spell.unknown(text.split())
    for word in text.split():
        if word in misspelled_words:
            corrected_text.append(spell.correction(word))
        else:
            corrected_text.append(word)
    return " ".join(corrected_text)

text = "corect me plese"
correct_spellings(text)

"""GloVe for Vectorization"""

# Tokenizes the text into words, filters out stopwords and non-alphabetical characters, and then returns a list of lists (a "corpus") where each inner list contains
# the cleaned and tokenized words from each tweet

def create_corpus(df):
    corpus=[]
    for tweet in tqdm(df['text']):
        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]
        corpus.append(words)
    return corpus

nltk.download('punkt')
corpus = create_corpus(df)

# Reads a pre-trained GloVe (Global Vectors for Word Representation) embedding file and loads the word embeddings into a dictionary, where each word is mapped to its
# corresponding embedding vector.
# GloVe pretrained corpus model to represent our words. It is available in 3 varieties :50D ,100D and 200 Dimentional.
# We will try 100 D here

embedding_dict={}
with open('/content/GloVe/GlobalVectors/glove.6B.100d.txt','r') as f:
    for line in f:
        values=line.split()
        word=values[0]
        vectors=np.asarray(values[1:],'float32')
        embedding_dict[word]=vectors
f.close()

# Tokenize the text data in a corpus, converts the text into sequences of integers, and then pads or truncates these sequences to ensure
# they all have a fixed length of 50 words.

MAX_LEN=50
tokenizer_obj=Tokenizer()
tokenizer_obj.fit_on_texts(corpus)
sequences=tokenizer_obj.texts_to_sequences(corpus)

tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')

word_index=tokenizer_obj.word_index
print('Number of unique words:',len(word_index))

# Initializes an embedding matrix by mapping each word in the vocabulary to its corresponding
# pre-trained embedding vector. Words not present in the pre-trained embeddings are represented by a zero vector.

num_words=len(word_index)+1
embedding_matrix=np.zeros((num_words,100))

for word,i in tqdm(word_index.items()):
    if i > num_words:
        continue

    emb_vec=embedding_dict.get(word)
    if emb_vec is not None:
        embedding_matrix[i]=emb_vec

# For binary classification, Sequential model is used which consists of an embedding layer initialized with
# pre-trained word embeddings, followed by a dropout layer, an LSTM layer, and a dense output layer with a
# sigmoid activation

model=Sequential()

embedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),
                   trainable=False)

model.add(embedding)
model.add(SpatialDropout1D(0.2))
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

optimzer=Adam(learning_rate=1e-5)

model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])

train=tweet_pad[:tweet.shape[0]]
test=tweet_pad[tweet.shape[0]:]

X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)
print('Shape of train',X_train.shape)
print("Shape of Validation ",X_test.shape)

# Train the LSTM-based Keras model on the training data for 15 epochs using a batch size of 4. It evaluates the model on validation data after each epoch, providing # real-time feedback on how the model is performing on unseen data. The training process's details, including the loss and accuracy for both training and validation # sets, are stored in the history object, which can be used for further analysis,
# such as plotting learning curves.

history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=1)

# Generate predictions for the test data using a trained model, processes these predictions into binary outcomes, and then creates a submission file formatted with an 'id' column and a 'target' column. The final submission file, submission.csv, is saved
# in the current directory

sample_sub=pd.read_csv('/content/GloVe/sample_submission.csv')
y_pre=model.predict(test)
y_pre=np.round(y_pre).astype(int).reshape(3263)
sub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})
sub.to_csv('submission.csv',index=False)

sub_df = pd.read_csv("/content/submission.csv")
sub_df.head(10)

# We are predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.