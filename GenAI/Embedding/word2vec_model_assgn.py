# -*- coding: utf-8 -*-
"""Word2Vec_Model_Assgn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e4wVSbJoPSkUxgbDM5hnPv31HhYF6jSD
"""



"""## Word2Vec - Word Embedding Model"""

# Importing all the necessary libraries
import re                                         # For preprocessing
import pandas as pd                               # For data handling
from time import time                             # To time our operations
from collections import defaultdict               # For word frequency
import spacy                                      # For preprocessing
import logging                                    # Setting up the loggings to monitor gensim
from gensim.models.phrases import Phrases, Phraser# To automatically detect common phrases (bigrams) from a list of sentences.
import multiprocessing                            # To create and manage separate process
from gensim.models import Word2Vec                # Word2Vec model
import numpy as np                                # For working with arrays and matrices
import matplotlib.pyplot as plt                   # For plotting graphs
import seaborn as sns                             # For creating attractive and informative statistical graphics
sns.set_style("darkgrid")                         # Setting dark background with grid lines to the plots
from sklearn.decomposition import PCA             # PCA (Principal Component Analysis) is used for dimensionality reduction
from sklearn.manifold import TSNE                 # t-SNE (t-Distributed Stochastic Neighbor Embedding) is used for visualizing high-dimensional data by reducing it to 2 or 3 dimensions

logging.basicConfig(format="%(levelname)s - %(asctime)s: %(message)s", datefmt= '%H:%M:%S', level=logging.INFO)

# Read the file
df = pd.read_csv('/content/simpsons_dataset.csv')
df.shape

df.head()

# Finding the missing values
df.isnull().sum()

# Removing the missing values:

df = df.dropna().reset_index(drop=True)
df.isnull().sum()

"""Cleaning"""

# Removing the stopwords and non-alphabetic characters for each line of dialogue
nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed

def cleaning(doc):
    # Lemmatizes and removes stopwords
    # doc needs to be a spacy Doc object
    txt = [token.lemma_ for token in doc if not token.is_stop]
    # Word2Vec uses context words to learn the vector representation of a target word,
    # if a sentence is only one or two words long,
    # the benefit for the training is very small
    if len(txt) > 2:
        return ' '.join(txt)

# Removes non-alphabetic characters
brief_cleaning = (re.sub("[^A-Za-z']+", ' ', str(row)).lower() for row in df['spoken_words'])

# Using spaCy .pipe() attribute to speed-up the cleaning process
t = time()

txt = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]

print('Time to clean up everything: {} mins'.format(round((time() - t) / 60, 2)))

# Putting the results in a DataFrame to remove missing values and duplicates:

df_clean = pd.DataFrame({'clean': txt})
df_clean = df_clean.dropna().drop_duplicates()
df_clean.shape

# As Phrases() takes a list of list of words as input
sent = [row.split() for row in df_clean['clean']]

# Create the relevant phrases from the list of sentences:
phrases = Phrases(sent, min_count=30, progress_per=10000)

# Cut down memory consumption of Phrases(), by discarding model state not strictly needed for the bigram detection task
bigram = Phraser(phrases)

# Transform the corpus based on the bigrams detected:
sentences = bigram[sent]

# Sanity check of the effectiveness of the lemmatization, removal of stopwords, and addition of bigrams.

word_freq = defaultdict(int)
for sent in sentences:
    for i in sent:
        word_freq[i] += 1
len(word_freq)

sorted(word_freq, key=word_freq.get, reverse=True)[:10]

"""Training the model"""

cores = multiprocessing.cpu_count() # Count the number of cores in a computer

# Word2Vec model
w2v_model = Word2Vec(min_count=20,
                     window=2,
                     vector_size=300,
                     sample=6e-5,
                     alpha=0.03,
                     min_alpha=0.0007,
                     negative=20,
                     workers=cores-1)

# Building the vocabulary table
t = time()

w2v_model.build_vocab(sentences, progress_per=10000)

print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))

# Training of the model
t = time()

w2v_model.train(sentences, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)

print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))

"""Exploring the model"""

# Asking the model to find the word most similar to some of the most iconic characters of the Simpsons
w2v_model.wv.most_similar(positive=["homer"])

# Checking for "homer_simpson"
w2v_model.wv.most_similar(positive=["homer_simpson"])

# Checking for "marge"
w2v_model.wv.most_similar(positive=["marge"])

# Checking  the similarity between ar are two words to each other
w2v_model.wv.similarity('maggie', 'baby')

w2v_model.wv.similarity('bart', 'nelson')

# Finding the odd one out
w2v_model.wv.doesnt_match(["nelson", "bart", "milhouse"])

w2v_model.wv.doesnt_match(['homer', 'patty', 'selma'])

# Analogy difference - Which word is to woman as homer is to marge
w2v_model.wv.most_similar(positive=["woman", "homer"], negative=["marge"], topn=3)

# Analogy difference - Which word is to woman as bart is to man
w2v_model.wv.most_similar(positive=["woman", "bart"], negative=["man"], topn=3)

"""Data Visualizations"""

# tSNE Visualizations
# Finding relationships between a query word (in **red**), its most similar words in the model (in **blue**), and other words from the vocabulary (in **green**).

def tsnescatterplot(model, word, list_names):
    """ Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,
    its list of most similar words, and a list of words.
    """
    arrays = np.empty((0, 300), dtype='f')
    word_labels = [word]
    color_list  = ['red']

    # adds the vector of the query word
    arrays = np.append(arrays, model.wv.__getitem__([word]), axis=0)

    # gets list of most similar words
    close_words = model.wv.most_similar([word])

    # adds the vector for each of the closest words to the array
    for wrd_score in close_words:
        wrd_vector = model.wv.__getitem__([wrd_score[0]])
        word_labels.append(wrd_score[0])
        color_list.append('blue')
        arrays = np.append(arrays, wrd_vector, axis=0)

    # adds the vector for each of the words from list_names to the array
    for wrd in list_names:
        wrd_vector = model.wv.__getitem__([wrd])
        word_labels.append(wrd)
        color_list.append('green')
        arrays = np.append(arrays, wrd_vector, axis=0)

    # Reduces the dimensionality from 300 to 50 dimensions with PCA
    reduc = PCA(n_components=15).fit_transform(arrays)

    # Finds t-SNE coordinates for 2 dimensions
    np.set_printoptions(suppress=True)

    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)

    # Sets everything up to plot
    df = pd.DataFrame({'x': [x for x in Y[:, 0]],
                       'y': [y for y in Y[:, 1]],
                       'words': word_labels,
                       'color': color_list})

    fig, _ = plt.subplots()
    fig.set_size_inches(9, 9)

    # Basic plot
    p1 = sns.regplot(data=df,
                     x="x",
                     y="y",
                     fit_reg=False,
                     marker="o",
                     scatter_kws={'s': 40,
                                  'facecolors': df['color'] })


# Adds annotations one by one with a loop
    for line in range(0, df.shape[0]):
         p1.text(df["x"][line],
                 df['y'][line],
                 '  ' + df["words"][line].title(),
                 horizontalalignment='left',
                 verticalalignment='bottom', size='medium',
                 color=df['color'][line],
                 weight='normal'
                ).set_size(15)


    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)
    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)

    plt.title('t-SNE visualization for {}'.format(word.title()))

# Vector representation of Homer, his 10 most similar words from the model, as well as 8 random ones
tsnescatterplot(w2v_model, 'homer', ['dog', 'bird', 'ah', 'maude', 'bob', 'mel', 'apu', 'duff'])

# 10 most similar words to Homer ends up around him

# Vector representation of Maggie and her 10 most similar words from the model lies compare to the
# vector representation of the 10 most dissimilar words to Maggie
tsnescatterplot(w2v_model, 'maggie', [i[0] for i in w2v_model.wv.most_similar(negative=["maggie"])])

# Maggie and her most similar words form a overlapping cluster from the most dissimilar words

# Most similar words to Mr. Burns ranked 1st to 10th versus the ones ranked 11th to 20th
tsnescatterplot(w2v_model, "mr_burn", [t[0] for t in w2v_model.wv.most_similar(positive=["mr_burn"], topn=20)][10:])

# All words are forming one cluster around "Mr Burns"