# -*- coding: utf-8 -*-
"""FastTextVsGloVeVsWord2Vec_Model_Assgn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q8NFmCM1T9vMpjf-NHdnqaTXw7GtGs7n
"""



"""## FastText Vs GloVe Vs Word2Vec"""

# Importing all the necessary libraries
from gensim.models import Word2Vec             # To create and work with Word2Vec models
from gensim.models import FastText             # To create and work with FastText models
from sklearn.manifold import TSNE              # For a dimensionality reduction technique
import matplotlib.pyplot as plt                # To visualize the word embeddings
import numpy as np                             # For working with arrays and matrices

# Toy dataset
toy_data = [
    "word embeddings are fascinating",
    "word2vec captures semantic relationships",
    "GloVe considers global context",
    "FastText extends Word2Vec with subword information"
]

# Function to train Word2Vec model
def train_word2vec(data):
    model = Word2Vec([sentence.split() for sentence in data], vector_size=100, window=5, min_count=1, workers=4)
    return model

# Reads a pre-trained GloVe (Global Vectors for Word Representation) embedding file and loads the word embeddings into a dictionary, where each word is mapped to its
# corresponding embedding vector.
# GloVe pretrained corpus model to represent our words. It is available in 3 varieties :50D ,100D and 200 Dimentional.
# We will try 100 D here
def train_glove(data):
    embedding_dict={}
    with open('/content/glove.6B.100d.txt','r') as f:
        for line in f:
            values=line.split()
            word=values[0]
            vectors=np.asarray(values[1:],'float32')
            embedding_dict[word]=vectors
    f.close()

    return embedding_dict

# Function to train FastText model
def train_fasttext(data):
    model = FastText(sentences=[sentence.split() for sentence in data], vector_size=100, window=5, min_count=1, workers=4)
    return model

# Function to plot embeddings
def plot_embeddings(model, title):
    labels = model.wv.index_to_key
    vectors = [model.wv[word] for word in labels]

    # Calculate perplexity based on the number of data points
    perplexity = min(20, len(vectors) - 1)

    tsne_model = TSNE(perplexity=perplexity, n_components=2, init='pca', n_iter=2500, random_state=42)
    new_values = tsne_model.fit_transform(np.array(vectors))

    x, y = [], []
    for value in new_values:
        x.append(value[0])
        y.append(value[1])

    plt.figure(figsize=(10, 8))
    for i in range(len(x)):
        plt.scatter(x[i],y[i])
        plt.annotate(labels[i],
                     xy=(x[i], y[i]),
                     xytext=(5, 2),
                     textcoords='offset points',
                     ha='right',
                     va='bottom')
    plt.title(title)
    plt.show()

# Train models
word2vec_model = train_word2vec(toy_data)
glove_model = train_glove(toy_data)
fasttext_model = train_fasttext(toy_data)

# Plot embeddings
plot_embeddings(word2vec_model, 'Word2Vec Embeddings')
# plot_embeddings(glove_model, 'GloVe Embeddings')
plot_embeddings(fasttext_model, 'FastText Embeddings')